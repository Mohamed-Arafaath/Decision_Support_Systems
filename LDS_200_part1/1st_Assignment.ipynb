{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import csv\n",
    "import json\n",
    "import pyodbc\n",
    "import requests\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant age, status, and type dictionaries\n",
    "with open('dict_partecipant_age.json') as f1:\n",
    "    dict_partecipant_age = json.load(f1)\n",
    "\n",
    "with open('dict_partecipant_status.json') as f2:\n",
    "    dict_partecipant_status = json.load(f2)\n",
    "\n",
    "with open('dict_partecipant_type.json') as f3:\n",
    "    dict_partecipant_type = json.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute additional date-related data\n",
    "def compute_date_data(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "    date = date_obj.date()\n",
    "    day = date_obj.day\n",
    "    month = date_obj.month\n",
    "    year = date_obj.year\n",
    "    quarter = (date_obj.month - 1) // 3 + 1\n",
    "    day_of_week = date_obj.strftime('%A')\n",
    "    return date, day, month, year, quarter, day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse dates.xml and create a mapping of date_fk to real date\n",
    "def parse_dates_xml(xml_file):\n",
    "    date_mapping = {}\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for row in root.findall('.//row'):\n",
    "        date = row.find('date').text\n",
    "        date_pk = int(row.find('date_pk').text)\n",
    "        date_mapping[date_pk] = date\n",
    "\n",
    "    return date_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute crime gravity using provided dictionaries\n",
    "def compute_crime_gravity(x):\n",
    "    gravity = dict_partecipant_age.get(x['participant_age_group'], 1) * \\\n",
    "              dict_partecipant_type.get(x['participant_type'], 1) * \\\n",
    "              dict_partecipant_status.get(x['participant_status'], 1)\n",
    "    return gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get location info using Google Maps Geocoding API with retry logic\n",
    "def get_location_info_with_retry(latitude, longitude, api_key, max_retries=3):\n",
    "    latitude = float(latitude)\n",
    "    longitude = float(longitude)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={latitude},{longitude}&key={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "\n",
    "            if data.get(\"results\"):\n",
    "                address_components = data[\"results\"][0][\"address_components\"]\n",
    "                city = next((component[\"long_name\"] for component in address_components if \"locality\" in component[\"types\"]), None)\n",
    "                state = next((component[\"long_name\"] for component in address_components if \"administrative_area_level_1\" in component[\"types\"]), None)\n",
    "\n",
    "                return {\"city\": city or None, \"state\": state}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error in geocoding request (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            time.sleep(1)  # Adding a delay before retrying\n",
    "\n",
    "    return {\"city\": None, \"state\": None}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to my db\n",
    "# Connection string\n",
    "server = 'tcp:lds.di.unipi.it'\n",
    "username = 'Group_ID_200'\n",
    "password = '89VIG10K'\n",
    "database = 'Group_ID_200_DB'\n",
    "connectionString = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + server + ';DATABASE=' + database + ';UID=' + username + ';PWD=' + password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQL Server database\n",
    "conn = pyodbc.connect(connectionString)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data into the database with ID\n",
    "def insert_data_with_ID(conn, cursor, id_dict, table_name, key_dict):\n",
    "    key_tuple = tuple(key_dict.values())\n",
    "    if key_tuple not in id_dict:\n",
    "        id_dict[key_tuple] = next(iter(key_dict.values()))\n",
    "        columns = ', '.join(key_dict.keys())\n",
    "        placeholders = ', '.join(['?'] * len(key_dict))\n",
    "        insert_query = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders});'\n",
    "\n",
    "        cursor.execute(insert_query, list(key_dict.values()))\n",
    "        #conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data into the database without (ID's are created automatically - Surrogate key)\n",
    "def insert_data_without_ID(conn, cursor, table_name, key_dict, identity_column='YourIdentityColumn'):\n",
    "    columns = ', '.join(key_dict.keys())\n",
    "    placeholders = ', '.join(['?'] * len(key_dict))\n",
    "    insert_query = f'INSERT INTO {table_name} ({columns}) OUTPUT INSERTED.{identity_column} VALUES ({placeholders});'\n",
    "\n",
    "    # Execute the INSERT query and fetch the last inserted row's ID\n",
    "    cursor.execute(insert_query, list(key_dict.values()))\n",
    "    id_generated = cursor.fetchone()[0]\n",
    "\n",
    "    # Return the last inserted row's ID\n",
    "    return id_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_insert_id(conn, cursor, id_dict, table_name, key_dict, identity_column='IdentityColumn_id'):\n",
    "    key_tuple = tuple(key_dict.values())\n",
    "    if key_tuple in id_dict:\n",
    "        return id_dict[key_tuple]\n",
    "    else:\n",
    "        last_inserted_id = insert_data_without_ID(conn, cursor, table_name, key_dict, identity_column)\n",
    "        id_dict[key_tuple] = last_inserted_id\n",
    "        return last_inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_id_dict = {}\n",
    "gun_id_dict = {}\n",
    "partecipant_id_dict = {}\n",
    "date_id_dict = {}\n",
    "incident_id_dict = {}\n",
    "custody_id_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_mapping = parse_dates_xml('dates.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from Police.csv and imsert appropriate data into the tables in Database except city, state which is inserted later\n",
    "def split_and_integrate(csv_file):\n",
    "    # List of table names in your database\n",
    "    table_names = ['Custody', 'Geography', 'Gun', 'Date', 'Incident', 'Partecipant']\n",
    "\n",
    "    # Clean the tables by deleting all records\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f'DELETE FROM {table_name}')\n",
    "        conn.commit()\n",
    "\n",
    "    # Read and process Police.csv\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "\n",
    "        # Set the batch size\n",
    "        batch_size = 10000\n",
    "\n",
    "        # Counter to track the number of processed rows\n",
    "        row_count = 0\n",
    "      \n",
    "        for row in reader:\n",
    "                try:\n",
    "                    custody_id, participant_age_group, participant_gender, participant_status, participant_type, latitude, longitude, gun_stolen, gun_type, incident_id, date_fk = row\n",
    "\n",
    "                    gun_stolen_bool = 1 if row[\"gun_stolen\"] == 'Stolen' else 0\n",
    "                    gun_key_dict = {\"is_stolen\": gun_stolen_bool, \"gun_type\": row['gun_type']}\n",
    "                    gun_id = get_or_insert_id(conn, cursor, gun_id_dict, 'Gun', gun_key_dict, \"gun_id\")\n",
    "\n",
    "\n",
    "                    partecipant_key = {\n",
    "                        \"age_group\" : row['participant_age_group'], \n",
    "                        \"gender\" : row['participant_gender'], \n",
    "                        \"type\" : row['participant_type'], \n",
    "                        \"status\" : row['participant_status']\n",
    "                        }\n",
    "                    partecipant_id = get_or_insert_id(conn, cursor, partecipant_id_dict, 'Partecipant', partecipant_key, \"partecipant_id\")\n",
    "\n",
    "                    \n",
    "                    latitude, longitude = float(row['latitude']), float(row['longitude'])\n",
    "                    country = \"United States\"\n",
    "                    continent = \"North America\"\n",
    "                    geo_key = {\n",
    "                        \"latitude\" : str(latitude), \"longitude\" : str(longitude), \"country\" : country, \"continent\" : continent\n",
    "                        }\n",
    "                    geo_id = get_or_insert_id(conn, cursor, geo_id_dict, 'Geography', geo_key, \"geo_id\")\n",
    "\n",
    "\n",
    "                    # Normal ID, No Incremental Tables:\n",
    "                    date_id = int(row['date_fk'])\n",
    "                    date_value = date_mapping[date_id]\n",
    "                    date, day, month, year, quarter, day_of_week = compute_date_data(date_value)\n",
    "                    date_key ={\n",
    "                        \"date_id\": date_id, \"the_date\" : date, \"the_day\" : day, \"the_month\" : month, \"the_year\" : year, \"quarter\" : quarter, \"day_of_week\" : day_of_week\n",
    "                        }\n",
    "                    insert_data_with_ID(conn, cursor, date_id_dict, \"Date\", date_key)\n",
    "                \n",
    "\n",
    "                    incident_id = int(row['incident_id'])\n",
    "                    incident_key = {\"incident_id\" : incident_id}\n",
    "                    insert_data_with_ID(conn, cursor, incident_id_dict, \"Incident\", incident_key)\n",
    "                    \n",
    "\n",
    "                    custody_id = row['custody_id']\n",
    "                    custody_key = {\n",
    "                        \"custody_id\" : custody_id, \"partecipant_id\" : partecipant_id, \"gun_id\" : gun_id, \"geo_id\" : geo_id, \"date_id\" : date_id, \"crime_gravity\" : \n",
    "                        compute_crime_gravity(row), \"incident_id \" : incident_id\n",
    "                        }\n",
    "                    insert_data_with_ID(conn, cursor, custody_id_dict, 'Custody', custody_key)\n",
    "                 \n",
    "\n",
    "                    # Increment the row count\n",
    "                    row_count += 1\n",
    "    \n",
    "                    if row_count % batch_size == 0:\n",
    "                        print(row_count)\n",
    "                        conn.commit()\n",
    "                    \n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {row_count}: {e}\")\n",
    "                    # If an error occurs, rerun from the same row\n",
    "                    csvfile.seek(0)  # Reset the file pointer to the beginning\n",
    "                    next(reader)  # Skip the header row\n",
    "                    for _ in range(row_count):\n",
    "                        next(reader)\n",
    "        \n",
    "\n",
    "    # Commit any remaining records\n",
    "    conn.commit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the appropriate arguments\n",
    "split_and_integrate('Police.csv')\n",
    "\n",
    "# Close the database connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQL Server database\n",
    "conn = pyodbc.connect(connectionString)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Selecting all rows with city and state which are NULL\n",
    "query = \"SELECT latitude, longitude FROM Geography WHERE city IS NULL OR state IS NULL\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all records at once\n",
    "records = cursor.fetchall()\n",
    "print(len(records))\n",
    "\n",
    "# My Google Maps API key\n",
    "google_maps_api_key = \"AIzaSyBU-5iM3eGnShHFm0V1NFnkGmInJRysaOI\"\n",
    "\n",
    "# Batch size for committing changes\n",
    "batch_size = 1000\n",
    "row = 0\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing (adjust the number of threads as needed)\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for start_index in range(0, len(records), batch_size):\n",
    "        end_index = start_index + batch_size\n",
    "        batch_records = records[start_index:end_index]\n",
    "\n",
    "        # Get location info for the current batch\n",
    "        location_info_list = list(executor.map(lambda record: get_location_info_with_retry(*record, google_maps_api_key), batch_records))\n",
    "\n",
    "        print(\"Processing batch:\", start_index // batch_size + 1)\n",
    "        for record, info in zip(batch_records, location_info_list):\n",
    "            update_query = \"UPDATE Geography SET city = ?, state = ? WHERE latitude = ? AND longitude = ?\"\n",
    "            try:\n",
    "                cursor.execute(update_query, (info[\"city\"], info[\"state\"], *record))\n",
    "\n",
    "                # Fetch the updated record\n",
    "                select_query = \"SELECT * FROM Geography WHERE latitude = ? AND longitude = ?\"\n",
    "                cursor.execute(select_query, record)\n",
    "                updated_record = cursor.fetchone()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row}: {e}\")\n",
    "                break  # Exit the loop on error\n",
    "\n",
    "        conn.commit()\n",
    "        # Print progress\n",
    "        row += batch_size\n",
    "        print(\"Processed rows:\", row)\n",
    "    conn.commit() #Commit any remaining records\n",
    "    \n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
